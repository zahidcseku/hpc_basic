{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Your Local Machine: Leveraging HPC Clusters for Big Data ML Training\n",
    "\n",
    "**LinkedIn**: \n",
    "\n",
    "**Medium**: \n",
    "\n",
    "This notebook contains code corresponds to the article above. \n",
    "\n",
    "Imagine we are working on a social media analysis project. We have two datasets:\n",
    "\n",
    "1. An unlabeled dataset (**`test_data`**) (https://www.kaggle.com/datasets/sudishbasnet/truthseekertwitterdataset2023?select=Twitter+Analysis.csv) containing 60,000 rows, where each row represents a social media user with multiple features.\n",
    "2. A labeled dataset (**`train_data`**) (https://www.kaggle.com/datasets/danieltreiman/twitter-human-bots-dataset) with 25,000 rows.\n",
    "\n",
    "Our task is to label the instances in `test_data` using the $k$-nearest neighbors algorithm based on the `train_data`. Here's a simplified Python snippet of what this process might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37438, 5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "features = ['favourites_count', 'followers_count', 'friends_count', 'statuses_count']\n",
    "\n",
    "traindata = pd.read_csv(\"datasets/twitter_human_bots_dataset.csv\", \n",
    "                        usecols=features+['account_type'],\n",
    "                        index_col=False\n",
    "                       )\n",
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112566, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = pd.read_csv(\"datasets/Twitter Analysis.csv\", \n",
    "                       usecols=features+['BotScore'], \n",
    "                       index_col=False\n",
    "                      )\n",
    "testdata = testdata.drop_duplicates()\n",
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbour(query_vector, target_dataset, k=30):\n",
    "    # Perform a similarity search\n",
    "    index = faiss.IndexFlatL2(target_dataset.shape[1])\n",
    "    index.add(target_dataset)\n",
    "    \n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8ea3e83e5c4dc69fbcb8498433ba0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing the test dataset:   0%|          | 0/112566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import faiss\n",
    "\n",
    "labels = []\n",
    "for i, row in tqdm(testdata.iterrows(), total=len(testdata), desc=\"Processing the test dataset\"):\n",
    "    ids = find_neighbour(row[features].values.reshape(1, -1), traindata[features])\n",
    "    \n",
    "    labels.append(traindata.loc[ids[0], \"account_type\"].mode()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
